# -*- coding: utf-8 -*-
"""BDM_FinalC_wcl311.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RRmi6PSbPbjUEiHj4YdISaZu4HJiAFo8
"""

# Commented out IPython magic to ensure Python compatibility.
import csv
import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import IPython
# %matplotlib inline
IPython.display.set_matplotlib_formats('svg')
pd.plotting.register_matplotlib_converters()
sns.set_style("whitegrid")

import shapely
from shapely.geometry import Point
from pyproj import Transformer
import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.functions import udf, explode
from pyspark.sql.types import *
import json
from pyspark.sql import functions as F
from pyspark.sql import types as T
sc = pyspark.SparkContext.getOrCreate()
spark = SparkSession(sc)

from pyspark import SparkContext
import itertools

if __name__=='__main__':
  mart = spark.read.csv("nyc_supermarkets.csv",header='True')
  cbg = spark.read.csv("nyc_cbg_centroids.csv",header='True')
  wp = spark.read.option("header", True).option("escape", "\"").csv("weekly-patterns-nyc-2019-2020-sample.csv")

  wp_df = wp[['placekey','poi_cbg','visitor_home_cbgs','date_range_start','date_range_end']]
  mart = mart.withColumnRenamed('safegraph_placekey', 'placekey')
  wp_df = wp_df.join(mart,on=['placekey'],how='inner')

  wp_df = wp_df.withColumn("datetime_start",to_date("date_range_start"))
  wp_df = wp_df.withColumn("start",date_format("datetime_start", "yyyy/MM"))
  wp_df = wp_df.where(F.col("start").isin(['2019/03','2019/10','2020/03','2020/10']))

  def parser(element):
    return json.loads(element)
  jsonudf = udf(parser, MapType(StringType(), IntegerType()))
  visitor_home_cbgs_parsed = wp_df.withColumn("parsed_visitor_home_cbgs", jsonudf("visitor_home_cbgs"))
  visitor_home_cbgs_exploded = visitor_home_cbgs_parsed.select("poi_cbg", explode("parsed_visitor_home_cbgs"),'start','latitude','longitude')
  df = visitor_home_cbgs_exploded.selectExpr("poi_cbg as poi_cbg", "key as visitor_home_cbg","value as visitor_count","start as start",'latitude as poi_lat','longitude as poi_long')

  test = df.withColumnRenamed('visitor_home_cbg','cbg_fips')
  test = test.join(cbg,on='cbg_fips',how='inner')

  test = test.withColumnRenamed('latitude','vhc_lat')\
            .withColumnRenamed('longitude','vhc_long')
  test_pd = test.toPandas()
  test_pd = test_pd.astype({'poi_lat':'float', 'poi_long':'float','vhc_lat':'float', 'vhc_long':'float'})
  t = Transformer.from_crs(4326, 2263)

  poi_t=[]
  for i in range(1704):
    list1=[]
    list1.append(list(t.transform(test_pd.poi_lat[i],test_pd.poi_long[i])))
    poi_t.append(list1)

  vhc_t=[]
  for i in range(1704):
    list2=[]
    list2.append(list(t.transform(test_pd.vhc_lat[i],test_pd.vhc_long[i])))
    vhc_t.append(list2)
  
  distance=[]
  for i in range(1704):
    distance.append(Point(poi_t[i][0][0],poi_t[i][0][1]).distance(Point((vhc_t[i][0][0],vhc_t[i][0][1])))/5280)

  test_pd['distance'] = distance

  final = test_pd[['cbg_fips','poi_cbg','visitor_count','start','distance']]
  final['total_distance']  = final['distance']*final['visitor_count']
  df_final = final[['cbg_fips','visitor_count','total_distance','start']].groupby([final['cbg_fips'],final['start']]).agg({'visitor_count':'sum','total_distance':'sum'})
  df_final['average'] = df_final['total_distance'] / df_final['visitor_count']
  df_final['average'] = df_final['average'].round(2)

  df_final = df_final.reset_index()
  df_final = df_final.pivot(index='cbg_fips',columns='start',values='average').reset_index().rename_axis(None,axis=1)
  df_final = df_final.fillna('')
  df_final = df_final.set_index('cbg_fips')
  df_final["2019/03"] = pd.to_numeric(df_final["2019/03"],errors='coerce')
  df_final["2019/10"] = pd.to_numeric(df_final["2019/10"],errors='coerce')
  df_final["2020/03"] = pd.to_numeric(df_final["2020/03"],errors='coerce')
  df_final["2020/10"] = pd.to_numeric(df_final["2020/10"],errors='coerce')
  sf_final=spark.createDataFrame(df_final) 

  sf_final.coalesce(1).write.option('header','true').csv(sys.argv[1])

